# Employee Attrition Prediction Model

> **Note:** This documentation is provided in both English and Turkish. English version comes first, followed by the Turkish translation.
> 
> **Not:** Bu dokÃ¼mantasyon hem Ä°ngilizce hem de TÃ¼rkÃ§e olarak hazÄ±rlanmÄ±ÅŸtÄ±r. Ã–nce Ä°ngilizce versiyon, ardÄ±ndan TÃ¼rkÃ§e Ã§evirisi yer almaktadÄ±r.

---

# ğŸ‡¬ğŸ‡§ English Version

## ğŸ“‹ Project Overview

Hi, I'm **Emre**! This is my graduation project for the **Tech Istanbul Machine Learning Bootcamp**. The goal of this project is to predict employee attrition (whether an employee will leave the company or not) using various machine learning techniques. I wanted to build something that could actually be useful in a real HR department, so I focused on making the model not just accurate, but also interpretable and deployable.

Employee turnover is a significant challenge for companies - it costs time, money, and institutional knowledge. By predicting which employees are at risk of leaving, HR teams can take proactive measures to improve retention. That's the real-world problem I'm trying to solve here.

## ğŸ¯ Problem Statement

The objective is to develop a binary classification model that predicts whether an employee will leave the company (Attrition = "Yes") or stay (Attrition = "No"). The dataset comes from IBM HR Analytics, containing information about 1,470 employees with 35 different features covering demographics, job characteristics, and satisfaction metrics.

**Key Challenges:**
- Class imbalance (~16% attrition rate)
- Mix of numerical and categorical features
- Need for interpretable results (HR teams need to understand WHY)

## ğŸ› ï¸ Technologies Used

### Core Libraries

| Library | Version | Why I Chose It |
|---------|---------|----------------|
| **Python** | 3.10 | Stable version with great ML ecosystem support |
| **pandas** | 2.3.3 | Industry standard for data manipulation. The DataFrame structure makes EDA intuitive |
| **numpy** | 2.2.6 | Foundation for numerical computing. Essential for array operations |
| **scikit-learn** | 1.7.2 | The go-to library for classical ML. Has everything from preprocessing to model evaluation |
| **matplotlib** | 3.10.8 | Flexible plotting library. When I need full control over visualizations |
| **seaborn** | 0.13.2 | Built on matplotlib but makes statistical visualizations much easier |

### Advanced ML Libraries

| Library | Version | Why I Chose It |
|---------|---------|----------------|
| **XGBoost** | 3.1.2 | Gradient boosting powerhouse. Often wins Kaggle competitions. Great for tabular data |
| **LightGBM** | 4.6.0 | Faster training than XGBoost with similar performance. Uses histogram-based algorithms |
| **CatBoost** | 1.2.8 | Handles categorical features natively. No need for manual encoding |
| **imbalanced-learn** | 0.14.1 | Essential for handling class imbalance. SMOTE implementation is excellent |
| **SHAP** | 0.49.1 | Model interpretability is crucial for HR. SHAP values explain individual predictions |

### Why These Specific Choices?

1. **Multiple Boosting Libraries (XGBoost, LightGBM, CatBoost):** I wanted to compare different gradient boosting implementations. Each has its strengths - XGBoost is well-documented, LightGBM is fast, and CatBoost handles categories elegantly.

2. **SHAP for Interpretability:** In HR, you can't just say "the model predicts this person will leave." You need to explain WHY. SHAP provides local explanations for each prediction.

3. **imbalanced-learn:** With only 16% positive class, standard algorithms would be biased. SMOTE helps create a balanced training set.

## ğŸ“Š Dataset Description

**Source:** IBM HR Analytics Employee Attrition Dataset  
**Size:** 1,470 employees Ã— 35 features  
**Target Variable:** Attrition (Yes/No)

### Feature Categories

- **Demographics:** Age, Gender, MaritalStatus, Education
- **Job Characteristics:** Department, JobRole, JobLevel, YearsAtCompany
- **Compensation:** MonthlyIncome, StockOptionLevel, PercentSalaryHike
- **Satisfaction Metrics:** JobSatisfaction, EnvironmentSatisfaction, WorkLifeBalance
- **Work Patterns:** OverTime, BusinessTravel, DistanceFromHome

## ğŸ”§ Project Pipeline

```
1. Data Loading & Initial Exploration
         â†“
2. Exploratory Data Analysis (EDA)
   - Univariate Analysis
   - Bivariate Analysis
   - Correlation Analysis
   - Statistical Significance Tests
         â†“
3. Feature Engineering (25 new features created!)
   - Tenure ratios
   - Satisfaction indices
   - Career progression metrics
   - Risk indicators
         â†“
4. Preprocessing Pipeline
   - Numerical: StandardScaler
   - Nominal Categorical: OneHotEncoder
   - Ordinal Categorical: OrdinalEncoder
         â†“
5. Handling Class Imbalance (SMOTE)
         â†“
6. Model Development (12 algorithms tested)
         â†“
7. Hyperparameter Tuning (RandomizedSearchCV)
         â†“
8. Model Evaluation & Selection
         â†“
9. Ensemble Methods (Voting, Stacking)
         â†“
10. Model Interpretation (SHAP)
         â†“
11. Business Insights & Recommendations
         â†“
12. Model Export for Deployment
```

## ğŸ“ˆ Models Evaluated

I tested 12 different classification algorithms:

1. Logistic Regression
2. Random Forest
3. XGBoost
4. LightGBM
5. CatBoost
6. Gradient Boosting
7. AdaBoost
8. Support Vector Machine (SVM)
9. K-Nearest Neighbors (KNN)
10. Naive Bayes
11. Decision Tree
12. Extra Trees

### Evaluation Metrics

Since we have class imbalance, accuracy alone isn't enough. I focused on:

- **ROC-AUC:** Overall discriminative ability
- **Recall:** Important because missing an employee who will leave is costly
- **Precision:** Avoiding false alarms that waste HR resources
- **F1-Score:** Balance between precision and recall

## ğŸš€ How to Run

### Prerequisites

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\Activate.ps1

# Activate (Linux/Mac)
source venv/bin/activate
```

### Install Dependencies

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm catboost imbalanced-learn shap jupyter ipykernel jinja2
```

### Run the Notebook

```bash
jupyter notebook Project.ipynb
```

Or open directly in VS Code with the Jupyter extension.

## ğŸ“ Project Structure

```
KaggleProject/
â”œâ”€â”€ Project.ipynb              # Main notebook with full analysis
â”œâ”€â”€ README.md                  # This documentation
â”œâ”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv  # Dataset
â”œâ”€â”€ attrition_model.joblib     # Trained model (for deployment)
â”œâ”€â”€ preprocessor.joblib        # Fitted preprocessing pipeline
â”œâ”€â”€ feature_importance.csv     # Feature importance rankings
â”œâ”€â”€ model_metadata.json        # Model configuration and metrics
â”œâ”€â”€ AI.MD                      # AI assistant notes
â”œâ”€â”€ catboost_info/             # CatBoost training logs
â””â”€â”€ venv/                      # Python virtual environment
```

## ğŸ“Š Key Findings

Based on my analysis:

1. **OverTime** is the strongest predictor - employees working overtime are much more likely to leave
2. **Job Satisfaction** and **Environment Satisfaction** matter significantly
3. **Years at Company** shows a non-linear relationship - very new and very senior employees behave differently
4. **Monthly Income** relative to job level affects attrition risk
5. **Work-Life Balance** is crucial, especially combined with overtime

## ğŸ’¡ Business Recommendations

1. **Monitor Overtime:** Implement policies to prevent burnout
2. **Regular Satisfaction Surveys:** Track job and environment satisfaction
3. **Career Development:** Clear progression paths reduce attrition
4. **Competitive Compensation:** Especially for high-performers at risk
5. **Early Warning System:** Use this model to flag at-risk employees

## ğŸ”® Future Improvements

- [ ] Deploy as a web application (Flask/FastAPI)
- [ ] Add real-time prediction API
- [ ] Implement model monitoring for drift detection
- [ ] A/B test HR interventions suggested by the model
- [ ] Collect more data to improve predictions

## ğŸ“œ License

This project is created for educational purposes as part of Tech Istanbul Machine Learning Bootcamp.

## ğŸ™ Acknowledgments

- **Tech Istanbul** for the excellent machine learning bootcamp
- **IBM** for providing the HR Analytics dataset
- The open-source community for the amazing libraries

---

# ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon

## ğŸ“‹ Proje Ã–zeti

Merhaba, ben **Emre**! Bu proje, **Tech Istanbul Makine Ã–ÄŸrenmesi Bootcamp** bitirme projem. Projenin amacÄ±, Ã§eÅŸitli makine Ã¶ÄŸrenmesi teknikleri kullanarak Ã§alÄ±ÅŸan kaybÄ±nÄ± (bir Ã§alÄ±ÅŸanÄ±n ÅŸirketten ayrÄ±lÄ±p ayrÄ±lmayacaÄŸÄ±nÄ±) tahmin etmek. GerÃ§ek bir Ä°K departmanÄ±nda kullanÄ±labilecek bir ÅŸey yapmak istedim, bu yÃ¼zden modelin sadece doÄŸru deÄŸil, aynÄ± zamanda yorumlanabilir ve Ã¼retime alÄ±nabilir olmasÄ±na odaklandÄ±m.

Ã‡alÄ±ÅŸan devir hÄ±zÄ± ÅŸirketler iÃ§in Ã¶nemli bir zorluk - zaman, para ve kurumsal bilgi kaybÄ±na neden oluyor. Hangi Ã§alÄ±ÅŸanlarÄ±n ayrÄ±lma riski altÄ±nda olduÄŸunu tahmin ederek, Ä°K ekipleri elde tutmayÄ± iyileÅŸtirmek iÃ§in proaktif Ã¶nlemler alabilir. Ä°ÅŸte Ã§Ã¶zmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ±m gerÃ§ek dÃ¼nya problemi bu.

## ğŸ¯ Problem TanÄ±mÄ±

AmaÃ§, bir Ã§alÄ±ÅŸanÄ±n ÅŸirketten ayrÄ±lÄ±p (Attrition = "Yes") ayrÄ±lmayacaÄŸÄ±nÄ± (Attrition = "No") tahmin eden bir ikili sÄ±nÄ±flandÄ±rma modeli geliÅŸtirmek. Veri seti IBM Ä°K Analitik'ten geliyor ve demografik bilgiler, iÅŸ Ã¶zellikleri ve memnuniyet metrikleri dahil 35 farklÄ± Ã¶zelliÄŸe sahip 1.470 Ã§alÄ±ÅŸan hakkÄ±nda bilgi iÃ§eriyor.

**Temel Zorluklar:**
- SÄ±nÄ±f dengesizliÄŸi (~%16 iÅŸten ayrÄ±lma oranÄ±)
- SayÄ±sal ve kategorik Ã¶zelliklerin karÄ±ÅŸÄ±mÄ±
- Yorumlanabilir sonuÃ§lara ihtiyaÃ§ (Ä°K ekipleri NEDEN'i anlamak istiyor)

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

### Temel KÃ¼tÃ¼phaneler

| KÃ¼tÃ¼phane | Versiyon | Neden SeÃ§tim |
|-----------|----------|--------------|
| **Python** | 3.10 | ML ekosistemi desteÄŸi mÃ¼kemmel olan kararlÄ± versiyon |
| **pandas** | 2.3.3 | Veri manipÃ¼lasyonu iÃ§in endÃ¼stri standardÄ±. DataFrame yapÄ±sÄ± EDA'yÄ± sezgisel yapÄ±yor |
| **numpy** | 2.2.6 | SayÄ±sal hesaplama temeli. Dizi iÅŸlemleri iÃ§in ÅŸart |
| **scikit-learn** | 1.7.2 | Klasik ML iÃ§in baÅŸvuru kÃ¼tÃ¼phanesi. Ã–n iÅŸlemeden model deÄŸerlendirmeye her ÅŸey var |
| **matplotlib** | 3.10.8 | Esnek Ã§izim kÃ¼tÃ¼phanesi. GÃ¶rselleÅŸtirmeler Ã¼zerinde tam kontrol gerektiÄŸinde |
| **seaborn** | 0.13.2 | matplotlib Ã¼zerine kurulu ama istatistiksel gÃ¶rselleÅŸtirmeleri Ã§ok daha kolay yapÄ±yor |

### Ä°leri ML KÃ¼tÃ¼phaneleri

| KÃ¼tÃ¼phane | Versiyon | Neden SeÃ§tim |
|-----------|----------|--------------|
| **XGBoost** | 3.1.2 | Gradient boosting gÃ¼cÃ¼. Kaggle yarÄ±ÅŸmalarÄ±nÄ± sÄ±k kazanÄ±r. Tablo verisi iÃ§in harika |
| **LightGBM** | 4.6.0 | XGBoost'tan daha hÄ±zlÄ± eÄŸitim, benzer performans. Histogram tabanlÄ± algoritmalar kullanÄ±yor |
| **CatBoost** | 1.2.8 | Kategorik Ã¶zellikleri doÄŸal olarak iÅŸliyor. Manuel kodlama gerek yok |
| **imbalanced-learn** | 0.14.1 | SÄ±nÄ±f dengesizliÄŸini ele almak iÃ§in ÅŸart. SMOTE implementasyonu mÃ¼kemmel |
| **SHAP** | 0.49.1 | Model yorumlanabilirliÄŸi Ä°K iÃ§in kritik. SHAP deÄŸerleri bireysel tahminleri aÃ§Ä±klÄ±yor |

### Neden Bu Ã–zel SeÃ§imler?

1. **Birden Fazla Boosting KÃ¼tÃ¼phanesi (XGBoost, LightGBM, CatBoost):** FarklÄ± gradient boosting implementasyonlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak istedim. Her birinin gÃ¼Ã§lÃ¼ yanlarÄ± var - XGBoost iyi dokÃ¼mante edilmiÅŸ, LightGBM hÄ±zlÄ±, CatBoost kategorileri zarif ÅŸekilde iÅŸliyor.

2. **Yorumlanabilirlik iÃ§in SHAP:** Ä°K'da sadece "model bu kiÅŸinin ayrÄ±lacaÄŸÄ±nÄ± tahmin ediyor" diyemezsiniz. NEDEN'i aÃ§Ä±klamanÄ±z gerekiyor. SHAP her tahmin iÃ§in yerel aÃ§Ä±klamalar saÄŸlÄ±yor.

3. **imbalanced-learn:** Sadece %16 pozitif sÄ±nÄ±fla, standart algoritmalar Ã¶nyargÄ±lÄ± olurdu. SMOTE dengeli bir eÄŸitim seti oluÅŸturmaya yardÄ±mcÄ± oluyor.

## ğŸ“Š Veri Seti AÃ§Ä±klamasÄ±

**Kaynak:** IBM Ä°K Analitik Ã‡alÄ±ÅŸan KaybÄ± Veri Seti  
**Boyut:** 1.470 Ã§alÄ±ÅŸan Ã— 35 Ã¶zellik  
**Hedef DeÄŸiÅŸken:** Attrition (Evet/HayÄ±r)

### Ã–zellik Kategorileri

- **Demografik:** YaÅŸ, Cinsiyet, MedeniDurum, EÄŸitim
- **Ä°ÅŸ Ã–zellikleri:** Departman, Ä°ÅŸRolÃ¼, Ä°ÅŸSeviyesi, ÅirkettekiYÄ±llar
- **Ãœcret:** AylÄ±kGelir, HisseSenediSeÃ§enekSeviyesi, MaaÅŸArtÄ±ÅŸYÃ¼zdesi
- **Memnuniyet Metrikleri:** Ä°ÅŸMemnuniyeti, OrtamMemnuniyeti, Ä°ÅŸYaÅŸamDengesi
- **Ã‡alÄ±ÅŸma DÃ¼zenleri:** FazlaMesai, Ä°ÅŸSeyahati, EvdenUzaklÄ±k

## ğŸ”§ Proje Ä°ÅŸ AkÄ±ÅŸÄ±

```
1. Veri YÃ¼kleme & Ä°lk KeÅŸif
         â†“
2. KeÅŸifsel Veri Analizi (EDA)
   - Tek DeÄŸiÅŸkenli Analiz
   - Ä°ki DeÄŸiÅŸkenli Analiz
   - Korelasyon Analizi
   - Ä°statistiksel AnlamlÄ±lÄ±k Testleri
         â†“
3. Ã–zellik MÃ¼hendisliÄŸi (25 yeni Ã¶zellik oluÅŸturuldu!)
   - GÃ¶rev sÃ¼resi oranlarÄ±
   - Memnuniyet endeksleri
   - Kariyer ilerleme metrikleri
   - Risk gÃ¶stergeleri
         â†“
4. Ã–n Ä°ÅŸleme Pipeline'Ä±
   - SayÄ±sal: StandardScaler
   - Nominal Kategorik: OneHotEncoder
   - Ordinal Kategorik: OrdinalEncoder
         â†“
5. SÄ±nÄ±f DengesizliÄŸini Ele Alma (SMOTE)
         â†“
6. Model GeliÅŸtirme (12 algoritma test edildi)
         â†“
7. Hiperparametre Ayarlama (RandomizedSearchCV)
         â†“
8. Model DeÄŸerlendirme & SeÃ§im
         â†“
9. Topluluk YÃ¶ntemleri (Voting, Stacking)
         â†“
10. Model Yorumlama (SHAP)
         â†“
11. Ä°ÅŸ Ä°Ã§gÃ¶rÃ¼leri & Ã–neriler
         â†“
12. DaÄŸÄ±tÄ±m iÃ§in Model DÄ±ÅŸa Aktarma
```

## ğŸ“ˆ DeÄŸerlendirilen Modeller

12 farklÄ± sÄ±nÄ±flandÄ±rma algoritmasÄ± test ettim:

1. Lojistik Regresyon
2. Random Forest
3. XGBoost
4. LightGBM
5. CatBoost
6. Gradient Boosting
7. AdaBoost
8. Destek VektÃ¶r Makinesi (SVM)
9. K-En YakÄ±n KomÅŸu (KNN)
10. Naive Bayes
11. Karar AÄŸacÄ±
12. Extra Trees

### DeÄŸerlendirme Metrikleri

SÄ±nÄ±f dengesizliÄŸi olduÄŸundan sadece doÄŸruluk yeterli deÄŸil. OdaklandÄ±ÄŸÄ±m metrikler:

- **ROC-AUC:** Genel ayÄ±rt edici yetenek
- **Recall:** AyrÄ±lacak bir Ã§alÄ±ÅŸanÄ± kaÃ§Ä±rmak maliyetli olduÄŸu iÃ§in Ã¶nemli
- **Precision:** Ä°K kaynaklarÄ±nÄ± boÅŸa harcayan yanlÄ±ÅŸ alarmlardan kaÃ§Ä±nmak
- **F1-Score:** Precision ve recall arasÄ±nda denge

## ğŸš€ NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r

### Ã–n KoÅŸullar

```bash
# Sanal ortam oluÅŸtur
python -m venv venv

# AktifleÅŸtir (Windows)
.\venv\Scripts\Activate.ps1

# AktifleÅŸtir (Linux/Mac)
source venv/bin/activate
```

### BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm catboost imbalanced-learn shap jupyter ipykernel jinja2
```

### Notebook'u Ã‡alÄ±ÅŸtÄ±r

```bash
jupyter notebook Project.ipynb
```

Veya VS Code'da Jupyter eklentisiyle doÄŸrudan aÃ§abilirsiniz.

## ğŸ“ Proje YapÄ±sÄ±

```
KaggleProject/
â”œâ”€â”€ Project.ipynb              # Tam analizli ana notebook
â”œâ”€â”€ README.md                  # Bu dokÃ¼mantasyon
â”œâ”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv  # Veri seti
â”œâ”€â”€ attrition_model.joblib     # EÄŸitilmiÅŸ model (daÄŸÄ±tÄ±m iÃ§in)
â”œâ”€â”€ preprocessor.joblib        # Fit edilmiÅŸ Ã¶n iÅŸleme pipeline'Ä±
â”œâ”€â”€ feature_importance.csv     # Ã–zellik Ã¶nem sÄ±ralamalarÄ±
â”œâ”€â”€ model_metadata.json        # Model konfigÃ¼rasyonu ve metrikleri
â”œâ”€â”€ AI.MD                      # AI asistan notlarÄ±
â”œâ”€â”€ catboost_info/             # CatBoost eÄŸitim loglarÄ±
â””â”€â”€ venv/                      # Python sanal ortamÄ±
```

## ğŸ“Š Temel Bulgular

Analizime gÃ¶re:

1. **FazlaMesai** en gÃ¼Ã§lÃ¼ tahmin edici - fazla mesai yapan Ã§alÄ±ÅŸanlarÄ±n ayrÄ±lma olasÄ±lÄ±ÄŸÄ± Ã§ok daha yÃ¼ksek
2. **Ä°ÅŸ Memnuniyeti** ve **Ortam Memnuniyeti** Ã¶nemli Ã¶lÃ§Ã¼de etkili
3. **Åirketteki YÄ±llar** doÄŸrusal olmayan bir iliÅŸki gÃ¶steriyor - Ã§ok yeni ve Ã§ok kÄ±demli Ã§alÄ±ÅŸanlar farklÄ± davranÄ±yor
4. Ä°ÅŸ seviyesine gÃ¶re **AylÄ±k Gelir** iÅŸten ayrÄ±lma riskini etkiliyor
5. **Ä°ÅŸ-YaÅŸam Dengesi** kritik, Ã¶zellikle fazla mesaiyle birleÅŸtiÄŸinde

## ğŸ’¡ Ä°ÅŸ Ã–nerileri

1. **Fazla Mesaiyi Ä°zle:** TÃ¼kenmiÅŸliÄŸi Ã¶nlemek iÃ§in politikalar uygula
2. **DÃ¼zenli Memnuniyet Anketleri:** Ä°ÅŸ ve ortam memnuniyetini takip et
3. **Kariyer GeliÅŸimi:** Net ilerleme yollarÄ± iÅŸten ayrÄ±lmayÄ± azaltÄ±r
4. **RekabetÃ§i Ãœcret:** Ã–zellikle risk altÄ±ndaki yÃ¼ksek performanslÄ±lar iÃ§in
5. **Erken UyarÄ± Sistemi:** Risk altÄ±ndaki Ã§alÄ±ÅŸanlarÄ± iÅŸaretlemek iÃ§in bu modeli kullan

## ğŸ”® Gelecek Ä°yileÅŸtirmeler

- [ ] Web uygulamasÄ± olarak daÄŸÄ±tÄ±m (Flask/FastAPI)
- [ ] GerÃ§ek zamanlÄ± tahmin API'si ekle
- [ ] Drift tespiti iÃ§in model izleme uygula
- [ ] Modelin Ã¶nerdiÄŸi Ä°K mÃ¼dahalelerini A/B test et
- [ ] Tahminleri iyileÅŸtirmek iÃ§in daha fazla veri topla

## ğŸ“œ Lisans

Bu proje, Tech Istanbul Makine Ã–ÄŸrenmesi Bootcamp kapsamÄ±nda eÄŸitim amaÃ§lÄ± oluÅŸturulmuÅŸtur.

## ğŸ™ TeÅŸekkÃ¼rler

- MÃ¼kemmel makine Ã¶ÄŸrenmesi bootcamp'i iÃ§in **Tech Istanbul**
- Ä°K Analitik veri setini saÄŸladÄ±ÄŸÄ± iÃ§in **IBM**
- Harika kÃ¼tÃ¼phaneler iÃ§in aÃ§Ä±k kaynak topluluÄŸu

---

**Author / Yazar:** Emre  
**Project:** Tech Istanbul Machine Learning Bootcamp - Graduation Project  
**Proje:** Tech Istanbul Makine Ã–ÄŸrenmesi Bootcamp - Bitirme Projesi  
**Date / Tarih:** 2026
